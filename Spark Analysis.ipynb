{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "import pandas as pd\n",
    "#os.environ['SPARK_HOME']=\"C:/Users/321ni/spark-2.4.3-bin-hadoop2.7/\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='jupyter'\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON_OPS']='lab'\n",
    "os.environ['PYSPARK_PYTHON']='python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\321ni\\\\spark-3.5.0-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "#findspark.init(\"C:/Users/321ni/spark-2.4.3-bin-hadoop2.7/\")\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"4g\").appName(\"sparkan\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvcdata2 = spark.read.format('csv').options(header='true',inferschema='true').load(\"nonull.csv\")\n",
    "#mvcdata2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvcdata2.createOrReplaceTempView(\"allData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1839003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvcdata2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_accidents_sql = \"SELECT YEAR(CAST(TO_DATE(CRASH_DATE,'MM/dd/yyyy') as DATE)),COUNT(*) FROM allData GROUP BY YEAR(CAST(TO_DATE(CRASH_DATE,'MM/dd/yyyy') as DATE))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_accidents_df = spark.sql(yearly_accidents_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+--------+\n",
      "|year(CAST(to_date(CRASH_DATE, MM/dd/yyyy) AS DATE))|count(1)|\n",
      "+---------------------------------------------------+--------+\n",
      "|                                               2018|  218580|\n",
      "|                                               2023|   78205|\n",
      "|                                               2022|   96526|\n",
      "|                                               2019|  197752|\n",
      "|                                               2020|  105892|\n",
      "|                                               2012|   85382|\n",
      "|                                               2017|  218404|\n",
      "|                                               2021|  103537|\n",
      "|                                               2015|  182772|\n",
      "|                                               2014|  172545|\n",
      "|                                               2016|  207678|\n",
      "|                                               2013|  171730|\n",
      "+---------------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yearly_accidents_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timely_accidents_sql = \"SELECT HOUR(CAST(CRASH_TIME as TIMESTAMP)) as TIME_OCCUR,COUNT(*) as TOTAL_COUNT FROM allData GROUP BY HOUR(CAST(CRASH_TIME as TIMESTAMP)) ORDER BY TOTAL_COUNT DESC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timely_accidents_df = spark.sql(timely_accidents_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|TIME_OCCUR|TOTAL_COUNT|\n",
      "+----------+-----------+\n",
      "|        16|     132323|\n",
      "|        17|     129358|\n",
      "|        14|     122880|\n",
      "|        15|     115021|\n",
      "|        18|     113619|\n",
      "|        13|     106502|\n",
      "|        12|     101502|\n",
      "|         8|     101097|\n",
      "|         9|      97602|\n",
      "|        11|      95489|\n",
      "|        19|      93527|\n",
      "|        10|      91399|\n",
      "|        20|      78292|\n",
      "|        21|      66432|\n",
      "|        22|      60428|\n",
      "|         0|      59473|\n",
      "|         7|      55564|\n",
      "|        23|      50610|\n",
      "|         6|      40308|\n",
      "|         1|      31639|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timely_accidents_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "month_year_time_sql = \"SELECT HOUR(CAST(CRASH_TIME as TIMESTAMP)) as TIME_OCCUR, \"\n",
    "month_year_time_sql += \" YEAR(CAST(TO_DATE(CRASH_DATE,'MM/dd/yyyy') as DATE)) YEAR,\"\n",
    "month_year_time_sql += \" MONTH(CAST(TO_DATE(CRASH_DATE,'MM/dd/yyyy') as DATE)) MONTH,\"\n",
    "month_year_time_sql += \"  COUNT(*) from allData GROUP BY TIME_OCCUR,MONTH,YEAR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+--------+\n",
      "|TIME_OCCUR|YEAR|MONTH|count(1)|\n",
      "+----------+----+-----+--------+\n",
      "|        11|2021|    7|     443|\n",
      "|         2|2021|    7|     209|\n",
      "|         0|2022|    2|     332|\n",
      "|         5|2021|   10|     215|\n",
      "|         8|2021|    2|     309|\n",
      "|         1|2020|    2|     225|\n",
      "|         1|2020|    3|     142|\n",
      "|         9|2019|   12|     822|\n",
      "|        10|2022|    4|     337|\n",
      "|         6|2022|    1|     250|\n",
      "|        16|2022|    9|     538|\n",
      "|        13|2019|   12|     907|\n",
      "|         8|2021|    7|     376|\n",
      "|        18|2020|    8|     575|\n",
      "|        11|2020|    4|     155|\n",
      "|         8|2020|    3|     530|\n",
      "|        14|2019|    5|    1249|\n",
      "|         8|2022|    2|     382|\n",
      "|        15|2021|    8|     567|\n",
      "|        12|2021|    5|     493|\n",
      "+----------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_year_time_df = spark.sql(month_year_time_sql)\n",
    "month_year_time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_year_sql = \"SELECT B1,YEAR(CAST(TO_DATE(CRASH_DATE,'MM/dd/yyyy') as DATE)) YEAR, COUNT(*) as COUNT from allData\"\n",
    "borough_year_sql += \" GROUP BY B1,YEAR ORDER BY B1,COUNT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|      B1|YEAR|COUNT|\n",
      "+--------+----+-----+\n",
      "|   BRONX|2012|10104|\n",
      "|   BRONX|2023|10862|\n",
      "|   BRONX|2022|14334|\n",
      "|   BRONX|2021|15525|\n",
      "|   BRONX|2020|16142|\n",
      "|   BRONX|2014|20816|\n",
      "|   BRONX|2013|20827|\n",
      "|   BRONX|2015|22498|\n",
      "|   BRONX|2016|25606|\n",
      "|   BRONX|2019|25614|\n",
      "|   BRONX|2017|26960|\n",
      "|   BRONX|2018|28687|\n",
      "|BROOKLYN|2023|23370|\n",
      "|BROOKLYN|2012|24448|\n",
      "|BROOKLYN|2022|27867|\n",
      "|BROOKLYN|2021|30483|\n",
      "|BROOKLYN|2020|31278|\n",
      "|BROOKLYN|2013|49241|\n",
      "|BROOKLYN|2014|49969|\n",
      "|BROOKLYN|2015|53368|\n",
      "+--------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "borough_year_df = spark.sql(borough_year_sql)\n",
    "borough_year_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_month_sql = \"SELECT B1,MONTH(CAST(TO_DATE(CRASH_DATE,'MM/dd/yyyy') as DATE)) MONTH, COUNT(*) as COUNT from allData\"\n",
    "borough_month_sql += \" GROUP BY B1,MONTH ORDER BY B1,COUNT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|      B1|MONTH|COUNT|\n",
      "+--------+-----+-----+\n",
      "|   BRONX|    2|17197|\n",
      "|   BRONX|    4|17499|\n",
      "|   BRONX|    1|18708|\n",
      "|   BRONX|    3|19313|\n",
      "|   BRONX|   11|19881|\n",
      "|   BRONX|   12|19907|\n",
      "|   BRONX|    5|20238|\n",
      "|   BRONX|    6|20248|\n",
      "|   BRONX|    9|20893|\n",
      "|   BRONX|    8|21099|\n",
      "|   BRONX|   10|21284|\n",
      "|   BRONX|    7|21708|\n",
      "|BROOKLYN|    2|36832|\n",
      "|BROOKLYN|    4|38607|\n",
      "|BROOKLYN|    1|40428|\n",
      "|BROOKLYN|    3|41982|\n",
      "|BROOKLYN|   12|43675|\n",
      "|BROOKLYN|   11|44394|\n",
      "|BROOKLYN|    5|44515|\n",
      "|BROOKLYN|    6|44907|\n",
      "+--------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "borough_month_df = spark.sql(borough_month_sql)\n",
    "borough_month_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/321ni/Desktop/Year 2023- 2024/Big Data C/Person_data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m person_data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43minferschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPerson_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/321ni/Desktop/Year 2023- 2024/Big Data C/Person_data.csv."
     ]
    }
   ],
   "source": [
    "person_data = spark.read.format('csv').options(header='true',inferschema='true').load(\"Person_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified and added vehicle type details, need to rerun\n",
    "merge_cols = ['COLLISION_ID','B1','NUMBER_OF_PERSONS_INJURED',\n",
    " 'NUMBER_OF_PERSONS_KILLED',\n",
    " 'NUMBER_OF_PEDESTRIANS_INJURED',\n",
    " 'NUMBER_OF_PEDESTRIANS_KILLED',\n",
    " 'NUMBER_OF_CYCLIST_INJURED',\n",
    " 'NUMBER_OF_CYCLIST_KILLED',\n",
    " 'NUMBER_OF_MOTORIST_INJURED',\n",
    " 'NUMBER_OF_MOTORIST_KILLED',\"VEHICLE_TYPE_CODE_1\",\"VEHICLE_TYPE_CODE_2\",\"VEHICLE_TYPE_CODE_3\",\"VEHICLE_TYPE_CODE_4\"\n",
    "             ,\"VEHICLE_TYPE_CODE_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_data = person_data.withColumnRenamed(\"COLLISION_ID\",\"COLLISION_ID_PERSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = person_data.join(mvcdata2[merge_cols],person_data.COLLISION_ID_PERSON == mvcdata2.COLLISION_ID,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_data.repartition(1).write.format('csv').options(header='true').save(\"person_merged_accident\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.createOrReplaceTempView(\"merged_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_SAFETY_EQUIPMENT = \"SELECT SAFETY_EQUIPMENT,COUNT(*) FROM merged_data GROUP BY SAFETY_EQUIPMENT\"\n",
    "distinct_PERSON_INJURY = \"SELECT PERSON_INJURY,COUNT(*) FROM merged_data GROUP BY PERSON_INJURY\"\n",
    "distinct_PERSON_TYPE = \"SELECT PERSON_TYPE,COUNT(*) FROM merged_data GROUP BY PERSON_TYPE\"\n",
    "distinct_EJECTION = \"SELECT EJECTION,COUNT(*) FROM merged_data GROUP BY EJECTION\"\n",
    "distinct_POSITION_IN_VEHICLE = \"SELECT POSITION_IN_VEHICLE,COUNT(*) FROM merged_data GROUP BY POSITION_IN_VEHICLE\"\n",
    "distinct_PERSON_SEX = \"SELECT PERSON_SEX,COUNT(*) FROM merged_data GROUP BY PERSON_SEX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_PED_LOCATION = \"SELECT PED_LOCATION,COUNT(*) FROM merged_data GROUP BY PED_LOCATION\"\n",
    "distinct_PED_ACTION = \"SELECT PED_ACTION,COUNT(*) FROM merged_data GROUP BY PED_ACTION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_SAFETY_EQUIPMENT_df = spark.sql(distinct_SAFETY_EQUIPMENT)\n",
    "distinct_SAFETY_EQUIPMENT_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_PERSON_INJURY_df = spark.sql(distinct_PERSON_INJURY)\n",
    "distinct_PERSON_INJURY_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_PERSON_TYPE_df = spark.sql(distinct_PERSON_TYPE)\n",
    "distinct_PERSON_TYPE_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_EJECTION_df = spark.sql(distinct_EJECTION)\n",
    "distinct_EJECTION_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_POSITION_IN_VEHICLE_df = spark.sql(distinct_POSITION_IN_VEHICLE)\n",
    "distinct_POSITION_IN_VEHICLE_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_PERSON_SEX_df = spark.sql(distinct_PERSON_SEX)\n",
    "distinct_PERSON_SEX_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_PED_LOCATION_df = spark.sql(distinct_PED_LOCATION)\n",
    "distinct_PED_LOCATION_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_PED_ACTION_df = spark.sql(distinct_PED_ACTION)\n",
    "distinct_PED_ACTION_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_gender_sql = \"SELECT PERSON_SEX, COUNT(*) FROM merged_data where POSITION_IN_VEHICLE = 'Driver' GROUP BY PERSON_SEX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_gender_df = spark.sql(driver_gender_sql)\n",
    "driver_gender_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributing_factor1_sql = \"SELECT CONTRIBUTING_FACTOR_1,COUNT(*) as Total from merged_data \"\n",
    "contributing_factor1_sql += \" WHERE CONTRIBUTING_FACTOR_1 IS NOT NULL GROUP BY CONTRIBUTING_FACTOR_1 ORDER BY Total DESC\"\n",
    "contributing_factor2_sql = \"SELECT CONTRIBUTING_FACTOR_2,COUNT(*) as Total from merged_data \"\n",
    "contributing_factor2_sql += \" WHERE CONTRIBUTING_FACTOR_2 IS NOT NULL GROUP BY CONTRIBUTING_FACTOR_2 ORDER BY Total DESC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributing_factor1_df = spark.sql(contributing_factor1_sql)\n",
    "contributing_factor1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributing_factor2_df = spark.sql(contributing_factor2_sql)\n",
    "contributing_factor2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributing_factor1_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributing_factor2_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of vehicles involved in collisions\n",
    "num_vehicle_sql = \"SELECT COLLISION_ID, COUNT(*) as total FROM merged_data WHERE POSITION_IN_VEHICLE = 'Driver' GROUP BY COLLISION_ID ORDER BY TOTAL DESC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vehicle_df = spark.sql(num_vehicle_sql)\n",
    "num_vehicle_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify_sql = \"SELECT * FROM allData WHERE COLLISION_ID = 4366219\"\n",
    "data_verify_df = spark.sql(data_verify_sql)\n",
    "data_verify_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
